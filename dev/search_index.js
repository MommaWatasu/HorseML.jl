var documenterSearchIndex = {"docs":
[{"location":"Manual/LossFunction/#LossFunction","page":"LossFunction","title":"LossFunction","text":"","category":"section"},{"location":"Manual/LossFunction/","page":"LossFunction","title":"LossFunction","text":"reduction specifies the reduction to apply to the output:","category":"page"},{"location":"Manual/LossFunction/","page":"LossFunction","title":"LossFunction","text":"none : do nothing and return the vector of losses.\nsum : return the sum of the losses.\nmean : return the mean of the losses.","category":"page"},{"location":"Manual/LossFunction/","page":"LossFunction","title":"LossFunction","text":"These functions must have both the input y and t as vectors.","category":"page"},{"location":"Manual/LossFunction/","page":"LossFunction","title":"LossFunction","text":"LossFunction.mse\nLossFunction.cee\nLossFunction.mae\nLossFunction.huber\nLossFunction.logcosh_loss\nLossFunction.poisson\nLossFunction.hinge\nLossFunction.smooth_hinge","category":"page"},{"location":"Manual/LossFunction/#HorseML.LossFunction.mse","page":"LossFunction","title":"HorseML.LossFunction.mse","text":"mse(y, t; reduction=\"mean\")\n\nMean Square Error. This is the expression:\n\nMSE(y t) = fracsum_i=1^n (t_i-y_i)^2n\n\n\n\n\n\n","category":"function"},{"location":"Manual/LossFunction/#HorseML.LossFunction.cee","page":"LossFunction","title":"HorseML.LossFunction.cee","text":"cee(y, t; reduction=\"mean\")\n\nCross Entropy Error. This is the expression:\n\nCEE(y t) = fracsum_i=1^n tln yn\n\n\n\n\n\n","category":"function"},{"location":"Manual/LossFunction/#HorseML.LossFunction.mae","page":"LossFunction","title":"HorseML.LossFunction.mae","text":"mae(y, t)\n\nMean Absolute Error. This is the expression:\n\nMAE(y t) = fracsum_i=1^n t_i-y_in\n\n\n\n\n\n","category":"function"},{"location":"Manual/LossFunction/#HorseML.LossFunction.huber","page":"LossFunction","title":"HorseML.LossFunction.huber","text":"huber(y, t; δ=1, reduction=\"mean\")\n\nHuber-Loss. If δ is large, it will be a function like mse, and if it is small, it will be a function like mae. This is the expression:\n\na = t_i-y_i \nHuber(y t) = frac1n sum_i=1^n left\nbeginarrayll\nfrac12a^2  (a leq delta) \ndelta(a-frac12delta)  (a gt delta)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/LossFunction/#HorseML.LossFunction.logcosh_loss","page":"LossFunction","title":"HorseML.LossFunction.logcosh_loss","text":"logcosh_loss(y, t; reduction=\"mean\")\n\nLog Cosh. Basically, it's mae, but if the loss is small, it will be close to mse. This is the expression:\n\nLogcosh(y t) = fracsum_i=1^n log(cosh(t_i-y_i))n\n\n\n\n\n\n","category":"function"},{"location":"Manual/LossFunction/#HorseML.LossFunction.poisson","page":"LossFunction","title":"HorseML.LossFunction.poisson","text":"Poisson(y, t; reduction=\"mean\")\n\nPoisson Loss, Distribution of predicted value and loss of Poisson distribution. This is the expression:\n\nPoisson(y t) = fracsum_i=1^n y_i-t_i ln yn\n\n\n\n\n\n","category":"function"},{"location":"Manual/LossFunction/#HorseML.LossFunction.hinge","page":"LossFunction","title":"HorseML.LossFunction.hinge","text":"hinge(y, t; reduction=\"mean\")\n\nHinge Loss, for SVM. This is the expression:\n\nHinge(y t) = fracsum_i=1^n max(1-y_it_i 0)n\n\n\n\n\n\n","category":"function"},{"location":"Manual/LossFunction/#HorseML.LossFunction.smooth_hinge","page":"LossFunction","title":"HorseML.LossFunction.smooth_hinge","text":"smooth_hinge(y, t; reduction=\"mean\")\n\nSmoothing Hinge Loss. This is the expression:\n\nsmoothHinge(y t) = frac1n sum_i=1^n left\nbeginarrayll\n0  (t_iy_i geq 1) \nfrac12(1-t_iy_i)^2  (0 lt t_iy_i lt 1) \nfrac12 - t_iy_i  (t_iy_i leq 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/Classification/#Classification","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"Manual/Classification/#Models","page":"Classification","title":"Models","text":"","category":"section"},{"location":"Manual/Classification/","page":"Classification","title":"Classification","text":"Classification.Logistic\nClassification.SVC","category":"page"},{"location":"Manual/Classification/#HorseML.Classification.Logistic","page":"Classification","title":"HorseML.Classification.Logistic","text":"Logistic(; alpha = 0.01, ni = 1000)\n\nLogistic Regression classifier.\n\nThis struct learns classifiers using multi class softmax. Parameter α indicates the learning rate, and ni indicates the number of learnings.\n\nExample\n\njulia> model = Logistic(alpha = 0.1)\nLogistic(0.1, 1000, Matrix{Float64}(undef, 0, 0))\n\njulia> fit!(model, x, ct)\n3×3 Matrix{Float64}:\n  1.80736  1.64037  -0.447735\n -1.27053  1.70026   2.57027\n  4.84966 -0.473835 -1.37582\n\njulia> println(predict(model, x))\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n\n\n\n\n\n","category":"type"},{"location":"Manual/Classification/#HorseML.Classification.SVC","page":"Classification","title":"HorseML.Classification.SVC","text":"SVC(; alpha=0.01, ni=1000)\n\nSupport Vector Machine Classifier.\n\nThis struct learns classifiers using One-Vs-Rest. One-Vs-Rest generates two-class classifiers divided into one class and the other classes using Logistic Regression, adopting the most likely one among all classifiers.\n\nParameter α indicates the learning rate, and ni indicates the number of learnings.\n\nExample\n\njulia> model = SVC()\nSVC(0.01, 1000, Logistic[])\n\njulia> fit!(model, ct)\n3-element Vector{Logistic}:\n Logistic(0.01, 1000, [0.8116709490679518 1.188329050932049; 1.7228257190036231 0.2771742809963788; -0.1519960725403138 2.1519960725403116])\n Logistic(0.01, 1000, [0.9863693439936144 1.0136306560063886; 0.8838433946106077 1.11615660538939; 1.4431044559203794 0.5568955440796174])\n Logistic(0.01, 1000, [1.262510641510418 0.7374893584895849; 0.5242383002319192 1.4757616997680822; 1.864635796779504 0.135364203220495])\n\njulia> println(predict(model, x))\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n\n\n\n\n\n","category":"type"},{"location":"Manual/Classification/#DecisionTree","page":"Classification","title":"DecisionTree","text":"","category":"section"},{"location":"Manual/Classification/#Models-2","page":"Classification","title":"Models","text":"","category":"section"},{"location":"Manual/Classification/","page":"Classification","title":"Classification","text":"Tree.DecisionTree\nTree.RandomForest","category":"page"},{"location":"Manual/Classification/#HorseML.Tree.DecisionTree","page":"Classification","title":"HorseML.Tree.DecisionTree","text":"DecisionTree(; alpha = 0.01)\n\nNormal DecisionTree. alpha specify the complexity of the model. If it's small, it's complicated, and if it's big, it's simple.\n\nExample\n\njulia> tree = DecisionTree()\nDecisionTree(0.01, Dict{Any, Any}(), Any[])\n\njulia> fit!(tree, x, t)\nDict{String, Any} with 5 entries:\n  \"left\"        => Dict{String, Any}(\"left\"=>Dict{String, Union{Nothing, Vector…\n  \"class_count\" => [8, 13, 16]\n  \"threshold\"   => 5.7\n  \"right\"       => Dict{String, Any}(\"left\"=>Dict{String, Union{Nothing, Vector…\n  \"feature_id\"  => 1\n\njulia> println(predict(tree, x))\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n\n\n\n\n\n","category":"type"},{"location":"Manual/Classification/#HorseML.Tree.RandomForest","page":"Classification","title":"HorseML.Tree.RandomForest","text":"RandomForest(nt; alpha = 0.01)\n\nRandomForest Model. nt is the number of trees, and alpha is the same as alpha in DecisionTree.\n\nExample\n\njulia> model = RandomForest(10)\nRandomForest(0.01, 10, DecisionTree[], Vector{Any}[], #undef)\n\njulia> fit!(model, x, t)\n10×1 Matrix{Int64}:\n 1\n 2\n 2\n 2\n 2\n 1\n 1\n 1\n 1\n 1\n\njulia> println(predict(model, x))\nAny[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n\n\n\n\n\n","category":"type"},{"location":"Manual/Classification/#VisualTool","page":"Classification","title":"VisualTool","text":"","category":"section"},{"location":"Manual/Classification/","page":"Classification","title":"Classification","text":"Tree.MV","category":"page"},{"location":"Manual/Classification/#HorseML.Tree.MV","page":"Classification","title":"HorseML.Tree.MV","text":"MV(path, forest; rounded=false, bg=\"#ffffff\", fc=\"#000000\", nc=\"#ffffff\", label=\"Tree\", fs=\"18\")\n\nMake DicisionTree and RandomForest Visual(make a dot file, see also Graphviz). The arguments are as follows:\n\npath : The full path of the dot file. The suffix must be .dot.\nforest : The model.\nrounded : If rounded is true, the nodes will be rounded.\nbg : Background color, type of this must be String.\nfc : Font color, type of this must be String.\nnc : Node color, type of this must be String.\nlabel : The label of the graph.\nfs : Font size, type of this must be String.\n\nExample\n\njulia> MV(\"/home/ubuntu/test.dot\", model, rounded = true)\n\n\n\n\n\n","category":"function"},{"location":"Manual/Classification/","page":"Classification","title":"Classification","text":"If you make the model created in DecisionTree Example visualized, it'll be like this: (Image: Tree Visualized)","category":"page"},{"location":"Tutorial/Classifiers/#Classifiers","page":"Classifiers","title":"Classifiers","text":"","category":"section"},{"location":"Tutorial/Classifiers/#Encoders","page":"Classifiers","title":"Encoders","text":"","category":"section"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"In order to convert label-like string data to teacher data that can be used by classifiers, you need to encode the data. The encoders can be used in the classification module.","category":"page"},{"location":"Tutorial/Classifiers/#Label-Encoder","page":"Classifiers","title":"Label Encoder","text":"","category":"section"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"First, convert string data to numeric data. At this time, the conversion rules for string data and numeric data are stored in the encoder. Therefore, after the prediction, you can see the prediction results by decoding with the same encoder. for more details, see LabelEncoder.","category":"page"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"using HorseML.Preprocessing\nusing HorseML.Classification\nusing HorseML.Classification: fit!, predict\n\nLE = LabelEncoder()\ntest_t = LE(test_t)#This data is splitted in the previous chapter.\ntrain_t = LE(train_t)","category":"page"},{"location":"Tutorial/Classifiers/#One-Hot-Encoder","page":"Classifiers","title":"One-Hot Encoder","text":"","category":"section"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"Next, let's ocnvert the data into One-Hot formst for make learning easier. for more details, see OneHotEncoder.","category":"page"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"OHE = OneHotEncoder()\ntest_t = OHE(test_t)\ntrain_t = OHE(train_t)","category":"page"},{"location":"Tutorial/Classifiers/#Logistic-Regression","page":"Classifiers","title":"Logistic Regression","text":"","category":"section"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"Let's create a classifier using the normalized Iris dataset. First of all, we will classify using Logistic Regression, a basic classifier.","category":"page"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"model = Logistic()\nfit!(model, train_x, train_t)\n\npredict(model, test_x)","category":"page"},{"location":"Tutorial/Classifiers/#SVC(Support-Vector-Machine-Classifier)","page":"Classifiers","title":"SVC(Support Vector Machine Classifier)","text":"","category":"section"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"When there are N classes, this classifier creates two-value classifiers that divides into one class and the other N-1 classes. at the time of prediction, all the classifiers generated predict, and the class that is predicted to have the highest probability of one class is the prediction result(This algorithm is called One-vs-Rest). Mutiple classifiers are created, but the code is not much different from Logistic Regression.","category":"page"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"model = SVC()\nfit!(model, train_x, train_t)","category":"page"},{"location":"Tutorial/Classifiers/#Accuracy-Score","page":"Classifiers","title":"Accuracy Score","text":"","category":"section"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"We've only trained and predicted until now, but it's necessary to know the accuracy when training models. Here, let's use the LossFunction module to know the accuracy score. We haven't just predicted the SVC model yet, let's use it.","category":"page"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"using HorseML.LossFunction\n\naccuracy_score(model, x, t) = mse(predict(model, x), t)\naccuracy_score(model, test_x, test_t)","category":"page"},{"location":"Tutorial/Classifiers/","page":"Classifiers","title":"Classifiers","text":"Amazing! So far, we have been able to finish from building basic models of regression and classification to calculating accuracy.","category":"page"},{"location":"Tutorial/Tree/#Tree","page":"Tree","title":"Tree","text":"","category":"section"},{"location":"Tutorial/Tree/#Decision-Tree","page":"Tree","title":"Decision Tree","text":"","category":"section"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"From here, let's build a slightly more advanced model. The first is decision tree analysis. This model creates a tree structure classifier by creating nodes that recursively divide data.","category":"page"},{"location":"Tutorial/Tree/#Train","page":"Tree","title":"Train","text":"","category":"section"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"Unlike other classifiers, this model doesn't require the data used for training to be in OneHot format, and it can be used as labels.","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"using HorseML.Preprocessing\n\nusing HorseML.Tree\nusing LearningHrse.Tree: fit!, predict\n\ndata = Matrix(dataloader(\"iris\"))\nx, t = data[:, 1:4], data[:, 5]\n\nmodel = DecisionTree()\nfit!(model, x, t)","category":"page"},{"location":"Tutorial/Tree/#Visualize","page":"Tree","title":"Visualize","text":"","category":"section"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"Some of you may have seen it on HorseML's README.md. Let's visualize the trained decision tree using Graphviz! First, let's make a dot file to visualize with Graphviz by the MV function.","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"MV(\"/home/ubuntu/Tree.dot\", model, rounded = true, bgcolor = \"#4aae4a\", label = \"Tree Model\")","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"Next, it's done by visualizing it using Graphviz!","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"#In command line, if you haven't installed Graphviz, install with `sudo apt add graphviz`\n$ dot -Tpng -o /home/ubuntu/Tree.png /home/ubuntu/Tree.dot","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"Let's look at Tree.png... (Image: Tree Visualized)","category":"page"},{"location":"Tutorial/Tree/#RandomForest","page":"Tree","title":"RandomForest","text":"","category":"section"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"RandomForest generates some DecisionTree(like SVC). What is different from the decision tree is that you need to specify the number of decision trees to generate.","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"model = RandomForest(10)\n\nfit!(model, x, t)","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"Visualization can be done in the same way.","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"paths = [\"/home/ubuntu/tree$i.dot\", for i in 1 : 10]\nMV(paths, model, rounded = true, bg = \"#4aae4a\", label = \"Forest Model\")","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"$ dot -Tpng -o /home/ubuntu/tree1.png /home/ubuntu/tree1.dot\n$ dot -Tpng -o /home/ubuntu/tree2.png /home/ubuntu/tree2.dot\n$ dot -Tpng -o /home/ubuntu/tree3.png /home/ubuntu/tree3.dot\n⋮","category":"page"},{"location":"Tutorial/Tree/","page":"Tree","title":"Tree","text":"warning: Warning\nThis function is broken in HorseML v0.3.2","category":"page"},{"location":"Manual/Regression/#Regression","page":"Regression","title":"Regression","text":"","category":"section"},{"location":"Manual/Regression/#Models","page":"Regression","title":"Models","text":"","category":"section"},{"location":"Manual/Regression/","page":"Regression","title":"Regression","text":"Regression.LinearRegression\nRegression.Lasso\nRegression.Ridge","category":"page"},{"location":"Manual/Regression/#HorseML.Regression.LinearRegression","page":"Regression","title":"HorseML.Regression.LinearRegression","text":"LinearRegression()\n\nClassic regression model. This struct has no parameter. If you want to use polynomial model, use Regression.make_design_matrix().\n\nsee also: make_design_matrix\n\nExample\n\njulia> x = [\n    16.862463771320925 68.10823385851712\n    15.382965696961577 65.4313485700859\n    8.916228406218375 53.92034559524475\n    10.560285659132695 59.17305391117168\n    12.142253214135884 62.28708207525656\n    5.362107221163482 43.604947901567414\n    13.893239446341777 62.44348617377496\n    11.871357065173395 60.28433066289655\n    29.83792267802442 69.22281924803998\n    21.327107214235483 70.15810991597944\n    23.852372696012498 69.81780163668844\n    26.269031430914108 67.61037566099782\n    22.78907104644012 67.78105545358633\n    26.73342178134947 68.59263965946904\n    9.107259141706415 56.565383817343495\n    29.38551885863976 68.1005579469209\n    7.935966787763017 53.76264777936664\n    29.01677894379809 68.69484161138638\n    6.839609488194577 49.69794758177567\n    13.95215840314148 62.058116579899085]; #These data are also used to explanations of other functions.\n\njulia> t = [169.80980778351542, 167.9081124078835, 152.30845618985222, 160.3110300206261, 161.96826472170756, 136.02842285615077, 163.98131131382686, 160.117817321485, 172.22758529098235, 172.21342437006865, 171.8939175591617, 169.83018083884602, 171.3878062674257, 170.52487535026015, 156.40282783981309, 170.6488327896672, 151.69267899906185, 172.32478221316322, 145.14365314788827, 163.79383292080666];\n\njulia> model = LinearRegression()\nLinearRegression(Float64[])\n\njulia> fit!(model, x, t)\n3-element Vector{Float64}:\n -0.04772448076255398\n  1.395963968616736\n 76.7817095600793\n\njulia> model(x)\n20-element Vector{Float64}:\n 171.05359766482795\n 167.38737053144575\n 151.62704681535598\n 158.88117658330424\n 163.15274911747872\n 137.3967419011542\n 163.28751869479999\n 160.3699086857777\n 171.99027166957023\n 173.70207799107243\n 173.10650291105486\n 169.9096820022986\n 170.31402414534642\n 171.25872436348817\n 155.31030802635905\n 170.44522606721017\n 151.45368882321284\n 171.29242257091374\n 145.83183688699864\n 162.74674475052848\n\n\n\n\n\n","category":"type"},{"location":"Manual/Regression/#HorseML.Regression.Lasso","page":"Regression","title":"HorseML.Regression.Lasso","text":"Lasso(; alpha = 0.1, tol = 1e-4, mi = 1e+8)\n\nLasso Regression structure. eEach parameters are as follows:\n\nalpha : leaarning rate.\ntol : Allowable error.\nmi : Maximum number of learning.\n\nExample\n\njulia> model = Lasso()\nLasso(Float64[], 0.1, 0.0001, 100000000)\n\njulia> fit!(model, x, t)\n3-element Vector{Float64}:\n   0.0\n   0.5022766549841176\n 154.43624186616267\n\njulia> model(x)\n20-element Vector{Float64}:\n 188.64541774549468\n 187.30088075704523\n 181.5191726873298\n 184.15748544986084\n 185.72158909964372\n 176.33798923891868\n 185.80014722707335\n 184.71565381947883\n 189.20524796663838\n 189.67502263476888\n 189.50409373058318\n 188.39535519538825\n 188.481083670683\n 188.88872347085172\n 182.8477136378307\n 188.64156231429416\n 181.43996475587224\n 188.9400571253936\n 179.39836073711297\n 185.6065850765288\n\n\n\n\n\n","category":"type"},{"location":"Manual/Regression/#HorseML.Regression.Ridge","page":"Regression","title":"HorseML.Regression.Ridge","text":"Ridge(alpha = 0.1)\n\nRidge Regression. alpha is the value multiplied by regularization term.\n\nExample\n\njulia> model = Ridge()\nRidge(Float64[], 0.1)\n\njulia> fit!(model, x, t)\n3-element Vector{Float64}:\n -0.5635468573581848\n  2.1185952951687614\n 40.334109796666425\n\njulia> model(x)\n20-element Vector{Float64}:\n 175.12510514593038\n 170.28763505842625\n 149.54478779081344\n 159.7466476176333\n 165.4525001910219\n 129.6935485937018\n 164.79709438985097\n 161.3621431448216\n 170.17418141858434\n 176.95192713546982\n 174.8078461898064\n 168.76930346791391\n 171.09202561187362\n 170.58861763111338\n 155.04089855305028\n 168.05151465675456\n 149.76311329450505\n 169.5183634524783\n 141.7695072903308\n 163.94744858842117\n\n\n\n\n\n","category":"type"},{"location":"Manual/Regression/#Other","page":"Regression","title":"Other","text":"","category":"section"},{"location":"Manual/Regression/","page":"Regression","title":"Regression","text":"Regression.fit!\nRegression.make_design_matrix","category":"page"},{"location":"Manual/Regression/#HorseML.Regression.fit!","page":"Regression","title":"HorseML.Regression.fit!","text":"fit!(model, x, t)\n\nx must be the number of data in the first dimension and the number of feature in the second dimension.\n\n\n\n\n\n","category":"function"},{"location":"Manual/Regression/#HorseML.Regression.make_design_matrix","page":"Regression","title":"HorseML.Regression.make_design_matrix","text":"make_design_matrix(x, dims)\n\nThis function return the design matrix.\n\nExample\n\njulia> make_design_matrix(x, dims = 2) |> size\n(20, 5)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#NeuralNetwork","page":"NeuralNetwork","title":"NeuralNetwork","text":"","category":"section"},{"location":"Manual/NeuralNetwork/#Basics","page":"NeuralNetwork","title":"Basics","text":"","category":"section"},{"location":"Manual/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"To build a neural network with LearningHorse, use the NetWork type.","category":"page"},{"location":"Manual/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"NetWork\n@epochs","category":"page"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.NetWork","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.NetWork","text":"NetWork(layers...)\n\nConnect multiple layers, and build a NeuralNetwork. NetWork also supports index. You can also add layers later using the add_layer!() Function.\n\nExample\n\njulia> N = NetWork(Dense(10=>5, relu), Dense(5=>1, relu))\n\njulia> N[1]\n\nDense(IO:10=>5, σ:relu)\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.@epochs","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.@epochs","text":"@epochs n ex\n\nThis macro cruns ex n times. Basically this is useful for learning NeuralNetwork. Even if there are any output during the progress, the progress bar won't disappear! It is always displayed in the bottom line of the output. When the process is finished, display Complete!.\n\nExample\n\njulia> function yes()\n           println(\"yes\")\n           sleep(0.1)\n       end\nyes (generic function with 1 method)\n\njulia> @epochs 10 yes()\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nyes\n  Complete!\n\n\n\n\n\n","category":"macro"},{"location":"Manual/NeuralNetwork/#Layers","page":"NeuralNetwork","title":"Layers","text":"","category":"section"},{"location":"Manual/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"Dense\nDenseσ\nConv\nDropout\nFlatten\nMaxPool\nMeanPool","category":"page"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Dense","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Dense","text":"Dense(in=>out, σ; set_w = \"Xavier\", set_b = zeros, high_accuracy=false)\n\nCrate a traditinal Dense layer, whose forward propagation is given by:     σ.(muladd(W, X, b)) The input of x should be a Vector of length in, (Sorry for you can't learn using batch. I'll implement)\n\nExample\n\njulia> D = Dense(5=>2, relu)\nDense(IO:5=>2, σ:relu)\n\njulia> D(rand(Float64, 5)) |> size\n(2,)\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Denseσ","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Denseσ","text":"Denseσ(in=>out, σ; set_w = \"Xavier\", set_b = zeros, high_accuracy=false)\n\nDense layer which you can learn the parametes of the activation function. The implementation perfectly matches Dense layer. It is assumued that the activation function will be passed as a structure, and the parameters to be learned must be in the w field.\n\nExample\n\njulia> D = Denseσ(5=>2, relu)\nDenseσ(IO:5=>2, σ:relu)\n\njulia> D(rand(Float64, 5)) |> size\n(2,)\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Conv","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Conv","text":"Conv(kernel, in=>out, σ; stride = 1, pading = 0, set_w = \"Xavier\")\n\nThis is the traditional convolution layer. kernel is a tuple of integers that specifies the kernel size, it must have one or two elements. And, in and out specifies number of input and out channels.\n\nThe input data must have a dimensions WHCB(weight, width, channel, batch). If you want to use a data which has a dimentions WHC, you must be add a dimentioins of B.\n\nstride and padding are single integers or tuple(stride is tuple of 2 elements, padding is tuple of 2 elements), and if you specifies KeepSize to padding, we adjust sizes of input and return a matrix which has the same sizes. set_w is Xavier or He, it decide a method to create a first parameter. This parameter is the same as Dense().\n\nExample\n\njulia> C = Conv((2, 2), 2=>2, relu)\nConvolution(k:(2, 2), IO:2 => 2, σ:relu)\n\njulia> C(rand(10, 10, 2, 5)) |> size\n(9, 9, 2, 5)\n\nwarning: Warning\nWhen you specidies same to padding, in some cases, it will be returned one size smaller. Because of its expression.\n\njulia> C = Conv((2, 2), 2=>2, relu, padding = KeepSize)\nConvolution(k:(2, 2), IO:2 => 2, σ:relu\n\njulia> C(rand(10, 10, 2, 5)) |> size\n(9, 9, 2, 5)\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Dropout","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Dropout","text":"Dropout(p)\n\nThis layer dropout the input data.\n\nExample\n\njulia> D = Dropout(0.25)\nDropout(0.25)\n\njulia> D(rand(10))\n10-element Array{Float64,1}:\n 0.0\n 0.3955865029078952\n 0.8157710047424143\n 1.0129613533211907\n 0.8060508293474877\n 1.1067504108970596\n 0.1461289547292684\n 0.0\n 0.04581776023870532\n 1.2794087133638332\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Flatten","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Flatten","text":"Flatten()\n\nThis layer change the dimentions Image to Vector.\n\nExample\n\njulia> F = Flatten()\nFlatten(())\n\njulia> F(rand(10, 10, 2, 5)) |> size\n(1000, )\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.MaxPool","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.MaxPool","text":"MaxPool(k::NTuple; stride = k, padding = 0)\n\nThis is a layer for max pooling with kernel size k.\n\nExpects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(out).\n\nThe default stride is the same as kernel size k.\n\nExample\n\njulia> N = NetWork(Conv((2, 2), 5=>2, relu), MaxPool(2, 2))\nLayer1 : Convolution(k:(2, 2), IO:5=>2, σ:relu)\nLayer2 : MaxPool(k:(2, 2), stride:(2, 2) padding:(0, 0, 0, 0))\n\njulia> x = rand(Float64, 10, 10, 5, 5) |> size\n(10, 10, 5, 5)\n\njulia> N(x) |> size\n(4, 4, 5, 2)\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.MeanPool","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.MeanPool","text":"MeanPool(k::NTuple; stride = k, padding = 0)\n\nThis is a layer for mean pooling with kernel size k.\n\nExpects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(out).\n\nThe default stride is the same as kernel size k.\n\nExample\n\njulia> N = NetWork(Conv((2, 2), 5=>2, relu), MeanPool(2, 2))\nLayer1 : Convolution(k:(2, 2), IO:5=>2, σ:relu)\nLayer2 : MeanPool(k:(2, 2), stride:(2, 2) padding:(0, 0, 0, 0))\n\njulia> x = rand(Float64, 10, 10, 5, 5) |> size\n(10, 10, 5, 5)\n\njulia> N(x) |> size\n(4, 4, 5, 2)\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#Activations","page":"NeuralNetwork","title":"Activations","text":"","category":"section"},{"location":"Manual/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"σ\nhardσ\nhardtanh\nrelu\nleakyrelu\nrrelu\nprelu\nrelu6\nelu\ngelu\nswish\nselu\ncelu\nsoftplus\nsoftsign\nlogσ\nlogcosh\nmish\ntanhshrink\nsoftshrink\ntrelu\nlisht\ngaussian\nGCU\nSQU\nNCU\nSSU\nDSU","category":"page"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.σ","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.σ","text":"σ(x)\n\nStandard sigmoid activation function. Also, this function can be called with σ. This is the expression:\n\nsigma(x) = frac11+e^-x\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.hardσ","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.hardσ","text":"hardsigmoid(x) = max(0, min(1, (x + 2.5) / 6))\n\nPiecewise linear approximation of sigmoid. Also, this function can be called with hardσ. This is the expression:\n\nhardsigmoid(x) = left\nbeginarrayll\n1  (x geq frac14) \nfrac15 x  (- frac14 lt x lt frac14) \n0  (x leq - frac14)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.hardtanh","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.hardtanh","text":"hardtanh(x)\n\nLinear tanh function. This is the expression:\n\nhardtanh(x) = left\nbeginarrayll\n1  (x geq 1) \nx  (-1 lt x lt 1) \n-1  (x leq -1)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.relu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.relu","text":"relu(x) = max(0, x)\n\nrelu is Rectified Linear Unit. This is the expression:\n\nrelu(x) = left\nbeginarrayll\nx  (x geq 0) \n0  (x lt 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.leakyrelu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.leakyrelu","text":"leakyrelu(x; α=0.01) = (x>0) ? x : α*x\n\nLeaky Rectified Linear Unit. This is the expression:\n\nleakyrelu(x) = left\nbeginarrayll\nalpha x  (x lt 0) \nx  (x geq 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.rrelu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.rrelu","text":"rrelu(min, max)\n\nRandomized Rectified Linear Unit. The expression is the as leakyrelu, but α is a random number between min and max. Also, since this function is defined as a structure, use it as follows:\n\nDense(10=>5, rrelu(0.001, 0.1))\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.prelu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.prelu","text":"prelu(; α=0.01)\n\nParametric Ractified LinearUnit. The expression is the as leakyrelu, but α is determined by learning. Also, when using this function, use Denseσ instead of Dense.\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.relu6","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.relu6","text":"relu6(x)\n\nRelu function with an upper limit of 6. This is the expression:\n\nrelu6(x) = left\nbeginarrayll\n6  (x gt 6) \nx  (x geq 0) \n0  (x lt 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.elu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.elu","text":"elu(x, α=1)\n\nExponential Linear Unit activation function. You can also specify the coefficient explicitly, e.g. elu(x, 1). This is the expression:\n\nelu(x α) = left\nbeginarrayll\nx  (x geq 0) \nalpha(e^x-1)  (x lt 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.gelu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.gelu","text":"gelu(x)\n\nGaussian Error Linear Unit. This is the expression(phi is a distribution function of standard normal distribution.):\n\ngelu(x) = xphi(x)\n\nHowever, in the implementation, it is calculated with the following expression.\n\nsigma(x) = frac11+e^-x \ngelu(x) = xsigma(1702x)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.swish","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.swish","text":"swish(x; β=1)\n\nThe swish function. This is the expression:\n\nsigma(x) = frac11+e^-x \nswish(x) = xsigma(beta x)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.selu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.selu","text":"selu(x)\n\nScaled exponential linear units. This is the expression\n\nlambda = 10507009873554804934193349852946 \nalpha = 16732632423543772848170429916717 \nselu(x) = lambda left\nbeginarrayll\nx  (x geq 0) \nalpha(e^x-1)  (x lt 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.celu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.celu","text":"celu(x; α=1)\n\nContinuously Differentiable Exponential Linear Unit. This is the expression:\n\nalpha = 1 \ncelu(x) = left\nbeginarrayll\nx  (x geq 0) \nalpha(e^fracxalpha-1)  (x lt 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.softplus","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.softplus","text":"softplus(x) = log(1 + exp(x))\n\nthe softplus activation function. This is the expression:\n\nsoftplus(x) = ln(1+e^x)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.softsign","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.softsign","text":"softsign(x) = x / (1+abs(x))\n\nThe softsign activation function. This is the expression:\n\nsoftsign(x) = fracx1+x\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.logσ","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.logσ","text":"logσ(x)\n\nlogarithmic sigmoid function. This is the expression:\n\nsigma(x) = frac11+e^-x \nlogsigmoid(x) = log(sigma(x))\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.logcosh","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.logcosh","text":"logcosh(x)\n\nLog-Cosh function. This is the expression:\n\nlogcosh(x) = log(cosh(x))\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.mish","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.mish","text":"mish(x) = x * tanh(softplus(x))\n\nThe mish function. This is the expression:\n\nsoftplus(x) = ln(1+e^x) \nmish(x) = xtanh(softplus(x))\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.tanhshrink","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.tanhshrink","text":"tanhshrink(x)\n\nShrink tanh function. This is the expression:\n\ntanhshrink(x) = 1-tanh(x)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.softshrink","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.softshrink","text":"softshrink(x; λ=0.5)\n\nThis is the expression:\n\nlambda=05 \nsoftshrink(x) = left\nbeginarrayll\nx-lambda  (x gt lambda) \n0  (-lambda leq x leq lambda) \nx+lambda  (x lt -lambda) \nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.trelu","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.trelu","text":"trelu(x; θ=1)\n\nThreshold gated Rectified Linear Unit. This is the expression:\n\ntheta = 1 \ntrelu(x) = left\nbeginarrayll\nx  (x gt 0) \n0  (x leq 0)\nendarray\nright\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.lisht","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.lisht","text":"lisht(x)\n\nThis is the expression:\n\nlisht(x) = xtanh(x)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.gaussian","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.gaussian","text":"gaussian(x)\n\nThe Gauss Function. This is the expression:\n\nGaussian(x) = e^-x^2\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.GCU","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.GCU","text":"GCU(x)\n\nGrowing Cosine Unit. This is the expression:\n\nGCU(x) = xcos(x)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.SQU","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.SQU","text":"SQU(x)\n\nShifted Quadratic Unit. SQU is a biologically inspired activation that enables single neurons to learn the XOR function. This is the expression:\n\nSQU(x) = x^2+x\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.NCU","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.NCU","text":"NCU(x)\n\nNon-Monotonic Cubic Unit. This is the expression:\n\nNCU(x) = x-x^3\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.SSU","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.SSU","text":"SSU(x)\n\nShifted Sinc Unit. This is the expression:\n\nSSU(x) = pi sinc(x-pi)\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.DSU","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.DSU","text":"DSU(x)\n\nDecaying Sine Unit. This is the expression:\n\nDSU(x) = fracpi2(sinc(x-pi)-sinc(x+pi))\n\n\n\n\n\n","category":"function"},{"location":"Manual/NeuralNetwork/#Optimizers","page":"NeuralNetwork","title":"Optimizers","text":"","category":"section"},{"location":"Manual/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"Descent\nMomentum\nAdaGrad\nAdam","category":"page"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Descent","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Descent","text":"Descent(η=0.1)\n\nBasic gradient descent optimizer with learning rate η.\n\nParameters\n\nlearning rate : η\n\nExample\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Momentum","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Momentum","text":"Momentum(η=0.01, α=0.9, velocity)\n\nMomentum gradient descent optimizer with learning rate η and parameter of velocity α.\n\nParameters\n\nlearning rate : η\nparameter of velocity : α\n\nExample\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.AdaGrad","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.AdaGrad","text":"AdaGrad(η = 0.01)\n\nGradient descent optimizer with learning rate attenuation.\n\nParameters\n\nη : initial learning rate\n\nExamples\n\n\n\n\n\n","category":"type"},{"location":"Manual/NeuralNetwork/#HorseML.NeuralNetwork.Adam","page":"NeuralNetwork","title":"HorseML.NeuralNetwork.Adam","text":"Adam(η=0.01, β=(0.9, 0.99))\n\nGradient descent adaptive moment estimation optimizer.\n\nParameters\n\nη : learning rate\nβ : Decay of momentums\n\nExamples\n\n\n\n\n\n","category":"type"},{"location":"Tutorial/NeuralNetwork/#NeuralNetwork","page":"NeuralNetwork","title":"NeuralNetwork","text":"","category":"section"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"Let's finally build the most powerful model, NeuralNetwork!","category":"page"},{"location":"Tutorial/NeuralNetwork/#Basic-NetWork","page":"NeuralNetwork","title":"Basic NetWork","text":"","category":"section"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"In anything, it's important to first learn the basics.　Let's create a networkconsisting only of Dense layer.","category":"page"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"using HorseML.Preprocessing\nusing HorseML.NeuralNetwork\nusing HorseML.LossFunction\n\ndata = Matrix(dataloader(\"iris\"))\nDS = DataSplitter(150, test_size = 0.3)\ntrain_data, test_data = DS(data)\ntrain_x, train_t = train_data[:, 1:4], train_data[:, 5]\ntrain_data = zip(train_x, train_t)\ntest_x, test_t = test_data[:, 1:4], test_data[:, 5]\n\nmodel = NetWork(Dense(4=>2, relu), Dense(2=>3, sigmoid))\nloss(x, y) = mse(model(x), y)\nopt = Adam()\ntrain!(model, loss, train_data, opt)\nloss(test_x, test_y)","category":"page"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"I wrote it all at once, but I'm doing is preparing the data, then definig and training the model.","category":"page"},{"location":"Tutorial/NeuralNetwork/#Advanced-Model","page":"NeuralNetwork","title":"Advanced Model","text":"","category":"section"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"Now that we know hot to build a network, let's build a model for image processing using the convolution layer.","category":"page"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"using HorseML.Preprocessing\nusing HorseML.NeuralNetwork\nusing HorseML.LossFunction\n\ntrain, test = Matrix(dataloader(\"MNIST\"))\ntrain = Matrix(train)\nx, t = reshape(train[:, 2:end], :, 28, 28, 1, 1), train[:, 1]\ntest_x, test_t = reshape(test_[:, 2:end], :, 28, 28, 1, 1), test_[:, 1]\nOHE = OneHotEncoder()\nt = OHE(t)\ndata = [(x[i, :, :, :, :], t[i, :]) for i in 1 : 60000]\n\nmodel = NetWork(Conv((3, 3), 1=>1, relu), MaxPool((2, 2)), Conv((2, 2), 1=>1, relu), MaxPool((2, 2)), Flatten(), Dense(36=>10, tanh))\nloss(x, y) = mse(model(x), y)\nopt = Adam()\n@epochs 10 train!(model, loss, data, opt)","category":"page"},{"location":"Tutorial/NeuralNetwork/#Create-your-layers","page":"NeuralNetwork","title":"Create your layers","text":"","category":"section"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"You can create your layer easily, like this:","category":"page"},{"location":"Tutorial/NeuralNetwork/","page":"NeuralNetwork","title":"NeuralNetwork","text":"#Layer definition\nstruct MyLayer\n    w::AbstractArray{Float64, 2}\n    b::AbstractVector{Float64}\n    σ\nend\n\n#This function is called during forward propergation\nfunction (L::MyLayer)(X::AbstractVecOrMat)\n    W, b, σ = L.w, L.b, L.σ\n    σ.(muladd(X, W, b))\nend\n\n#specify params\ntrainable(L::MyLayer) = L.w, L.b","category":"page"},{"location":"Tutorial/Getting_Started/#The-First-Step","page":"Getting Started","title":"The First Step","text":"","category":"section"},{"location":"Tutorial/Getting_Started/#First-Model","page":"Getting Started","title":"First Model","text":"","category":"section"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"First, let's try the simplest model, the linear regression model. The code is:","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"using HorseML.Regression\nusing HorseML.Regression: fit!, predict\n\n#data\nx = 5 .+ 25 .* rand(20)\nt = 170 .- 108 .* exp.(-0.2 .* x) .+ 4 .* rand(20)\nx1 = 23 .* (t ./ 100).^2 .+ 2 .* rand(20)\ntrain_data = hcat(x, x1)\n\n#model\nmodel = LinearRegression()\nfit!(model, train_data, t)","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"Just this, you can train the simple model. Let's make predictions too.","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"#generate new data\nx = 5 .+ 25 .* rand(20)\nx1 = 23 .* (t ./ 100).^2 .+ 2 .* rand(20)\ntest_data = hcat(x, x1)\n\npredict(model test_data)","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"Congratulations! You was able to train your first model using HorseML and use it to predict it.","category":"page"},{"location":"Tutorial/Getting_Started/#Other-Regression-Models","page":"Getting Started","title":"Other Regression Models","text":"","category":"section"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"Let's build another regression model.","category":"page"},{"location":"Tutorial/Getting_Started/#Polynomial-Regression","page":"Getting Started","title":"Polynomial Regression","text":"","category":"section"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"In many cases, the data is nonlinear. Let's use polunomial regression. With HorseML, you can do this by using make_design_matrix, which converts data to designed matrix.","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"The code of training is this:","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"using HorseML.Regression\nusing HorseML.Regression: fit!, predict\n\n#model\nmodel = LinearRegression()\nfit!(model, make_design_matrix(train_data), t)","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"Just pass the training data to make_design_matrix. Isn't it easy?","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"predicting is easy too:","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"predict(model, make_design_matrix(test_data))","category":"page"},{"location":"Tutorial/Getting_Started/#Ridge-Regression,-Lasso-Regression","page":"Getting Started","title":"Ridge Regression, Lasso Regression","text":"","category":"section"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"When building these models, you need to use different structures. But other than that, it's the same. Let's try it.","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"#Ridge Regression\nmodel = Ridge()\n\nfit!(model, make_design_matrix(train_data))\n\n#Lasso Regression\nmodel = Lasso()\n\nfit!(model, make_design_matrix(train_data))","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"Predicting is the same as polynomial regression.","category":"page"},{"location":"Tutorial/Getting_Started/#Saveing-and-Loading","page":"Getting Started","title":"Saveing and Loading","text":"","category":"section"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"There's no point in just learning the model you made, you need to save it and load it when you need it. HorseML itself doesn't implement this function, but it is very easy by using BSON.jl. after learning:","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"using BSON\nusing BSON: @save\n\n@save \"/home/ubuntu/model.bson\" model","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"and loading:","category":"page"},{"location":"Tutorial/Getting_Started/","page":"Getting Started","title":"Getting Started","text":"using BSON\nusing BSON: @load\n\n@load \"/home/ubuntu/model.bson\" model","category":"page"},{"location":"Tutorial/Welcome/#Welcome-to-HorseML!","page":"Welcome to HorseML","title":"Welcome to HorseML!","text":"","category":"section"},{"location":"Tutorial/Welcome/#Easy-to-use-and-very-functional-ML-Library","page":"Welcome to HorseML","title":"Easy-to-use and very functional ML Library","text":"","category":"section"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"HorseML is the ML Library for JuliaLang. With this package, you can build various models such as regression, classification, and decition tree, etc. ","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"In addition, each function is designed to be easy to use,  so it's good for those who want to learn machine learning with Julia!","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"Please try using HorseML!","category":"page"},{"location":"Tutorial/Welcome/#Development-Goals","page":"Welcome to HorseML","title":"Development Goals","text":"","category":"section"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"HorseML aims to be such a library.","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"less deps","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"I don't like deps. so it's basic ,but I'm trying to make it as few deps as possible.","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"Easy-to-use API","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"No matter how advanced the functions are, it's hard to use complicated things. I'm implementing it so that it's easy to understand.","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"Equipped with interesting functions","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"At the moment, I haven't reached this far because I have developed basic functions, but in the feature, we are thinking of implementing experimental functions that have never existed in other ML libraries!","category":"page"},{"location":"Tutorial/Welcome/#Installation","page":"Welcome to HorseML","title":"Installation","text":"","category":"section"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"From the Julia REPL, type ] to enter the Pkg REPL mode and run.","category":"page"},{"location":"Tutorial/Welcome/","page":"Welcome to HorseML","title":"Welcome to HorseML","text":"pkg> add HorseML","category":"page"},{"location":"Tutorial/Preprocessing/#Preprocessing","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"Tutorial/Preprocessing/#Data-Loader","page":"Preprocessing","title":"Data Loader","text":"","category":"section"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"With Preprocessing module, you can load the data. Also, some famous data sets can be loaded by specifying a name. First, let's load the MNIST data.","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"using HorseML.Preprocessing\n\ntrain_data, test_dara = dataloader(\"MNIST\")","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"If it is your first time to load a data set. you wil be asked if you want to download the data like this:","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Can I download the mnist_train.csv(110MB)? (y/n)","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"note: Note\nThe data downloaded here is in /home_directory/learningdatasets/ by default. for more details, see dataloader.","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Let's also read the local data. ","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"#Please create train.csv directly under the /home_directory/learningdatasets/ as follows:\n#0.202027,0.246752,0.608301,0.406351,0.0260402,0.747845\n#0.735292,0.332892,0.438458,0.0787028,0.797796,0.294831\n#0.710725,0.213594,0.527118,0.579191,0.298599,0.23684\n#0.288168,0.787194,0.809412,0.464031,0.960465,0.655897\ndf = dataloader(\"train.csv\", header = false)","category":"page"},{"location":"Tutorial/Preprocessing/#Data-Preprocessing","page":"Preprocessing","title":"Data Preprocessing","text":"","category":"section"},{"location":"Tutorial/Preprocessing/#Normalization","page":"Preprocessing","title":"Normalization","text":"","category":"section"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Even if you use the data as it is, you won't be able to make a model with high accuracy, let's normalize the data. The following three scalers are available in HorseML:","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Standard Scaler : tildeboldsymbolx = fracx_i-musigma\nMinMax Scaler : tildeboldsymbolx = fracboldsymbolx-min(boldsymbolx)max(boldsymbolx)-min(boldsymbolx)\nRobust Scaler : tildeboldsymbolx = fracboldsymbolx-Q2Q3 - Q1","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"For example, if you normalize iris data set with Standard Scaler,","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"using HorseML.Preprocessing: fit!\ndata = Matrix(dataloader(\"iris\"))\nx, t = data[:, 1:4], data[:, 5]\n\nscaler = Standard()\nx = fit_transform!(scaler, x)","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Once the scaler is fittted, sclaes using the same value unless it fits again.","category":"page"},{"location":"Tutorial/Preprocessing/#Data-Spliting","page":"Preprocessing","title":"Data Spliting","text":"","category":"section"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Data division is done by DataSplitter.","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"warning: Warning\nthis function is broken in HorseML v0.3.2","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"DS = DataSplitter(150, test_size = 0.3)\n\ntrain_x, test_x = DS(x)\ntrain_t, test_t = DS(t)","category":"page"},{"location":"Tutorial/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"You can specify test_size and train_size, whether it's the number of data or the percentage.","category":"page"},{"location":"Manual/Preprocessing/#Preprocessing","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"Manual/Preprocessing/#Data-Preprocessing","page":"Preprocessing","title":"Data Preprocessing","text":"","category":"section"},{"location":"Manual/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.dataloader\nPreprocessing.DataSplitter","category":"page"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.dataloader","page":"Preprocessing","title":"HorseML.Preprocessing.dataloader","text":"dataloader(name; header=true, dir=\"HorseMLdatasets\")\n\nLoad a data for Machine Learning. name is either the name of the datasets or the full path of the data file to be loaded. The following three can be specified as the name of the dataset in name.\n\nMNIST : The MNIST Datasets\niris : The iris Datasets\nBostonHousing : Boston Housing DataSets\n\nAnd these datasets are downloaded and saved by creating a dir folder under the home directly(i.e. it is saved in the /home_directory/HorseMLdatasets by default). When importing a data file, you can specify whether to read the header with header.\n\nExample\n\njulia> dataloader(\"MNIST\");\n\njulia> dataloader(\"/home/ubuntu/data/data.csv\", header = false)\n\n\n\n\n\n","category":"function"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.DataSplitter","page":"Preprocessing","title":"HorseML.Preprocessing.DataSplitter","text":"DataSplitter(ndata; test_size=nothing, train_size=nothing)\n\nSplit the data into test data and training data. ndata is the number of the data, and you must specify either test_size or train_size. thease parameter can be proprtional or number of data.\n\nnote: Note\nIf both test_size and train_size are specified, test_size takes precedence.\n\n#Example\n\njulia> x = rand(20, 2);\n\njulia> DS = DataSplitter(50, train_size = 0.3);\n\njulia> train, test = DS(x, dims = 2);\n\njulia> train |> size\n(6, 2)\n\njulia> test |> size\n(14, 2)\n\n\n\n\n\n","category":"type"},{"location":"Manual/Preprocessing/#Encoders","page":"Preprocessing","title":"Encoders","text":"","category":"section"},{"location":"Manual/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.LabelEncoder\nPreprocessing.OneHotEncoder","category":"page"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.LabelEncoder","page":"Preprocessing","title":"HorseML.Preprocessing.LabelEncoder","text":"LabelEncoder()\n\nLabelEncoder structure. LE(label; count=false, decode=false) Convert labels(like string) to class numbers(encode), and convert class numbers to labels(decode).\n\nExample\n\njulia> label = [\"Apple\", \"Apple\", \"Pear\", \"Pear\", \"Lemon\", \"Apple\", \"Pear\", \"Lemon\"]\n8-element Vector{String}\n \"Apple\"\n \"Apple\"\n \"Pear\"\n \"Pear\"\n \"Lemon\"\n \"Apple\"\n \"Pear\"\n \"Lemon\"\n\njulia> LE = LabelEncoder()\nLabelEncoder(Dict{Any, Any}())\n\njulia> classes, count = LE(label, count=true) #Encode\n([3.0 3.0 … 1.0 2.0], [3.0 2.0 3.0])\n\njulia> LE(classes, decode=true) #Decode\n1×18 Matrix{String}:\n \"Apple\" \"Apple\" \"Pear\" \"Pear\" \"Lemon\" \"Apple\" \"Pear\" \"Lemon\"\n\n\n\n\n\n","category":"type"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.OneHotEncoder","page":"Preprocessing","title":"HorseML.Preprocessing.OneHotEncoder","text":"OneHotEncoder()\n\nconvert data to Ont-Hot format. If you specified decode, data will be decoded.\n\nExample\n\njulia> x = [4.9 3.0; 4.6 3.1; 4.4 2.9; 4.8 3.4; 5.1 3.8; 5.4 3.4; 4.8 3.4; 5.2 4.1; 5.5 4.2; 5.5 3.5; 4.8 3.0; 5.1 3.8; 5.0 3.3; 6.4 3.2; 5.7 2.8; 6.1 2.9; 6.7 3.1; 5.6 2.5; 6.3 2.5; 5.6 3.0; 5.6 2.7; 7.6 3.0; 6.4 2.7; 6.4 3.2; 6.5 3.0; 7.7 3.8; 7.2 3.2; 7.2 3.0; 6.3 2.8; 6.1 2.6; 6.3 3.4; 6.0 3.0; 6.9 3.1; 6.7 3.1; 5.8 2.7; 6.8 3.2; 6.3 2.5];#These data are also used to explanations of other functions.\n\njulia> t = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2];\n\njulia> OHE = OneHotEncoder()\nOneHotEncoder()\n\njulia> ct = OHE(t)\n37×3 Matrix{Float64}:\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 1.0 0.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n 0.0 1.0 0.0\n ⋮\n 0.0 1.0 0.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n 0.0 0.0 1.0\n\njulia> OHE(ct, decode = true)\n37-element Vector{Int64}:\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 2\n 2\n 2\n 2\n ⋮\n 2\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n 3\n\n\n\n\n\n","category":"type"},{"location":"Manual/Preprocessing/#Scaler","page":"Preprocessing","title":"Scaler","text":"","category":"section"},{"location":"Manual/Preprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Preprocessing.Standard\nPreprocessing.MinMax\nPreprocessing.Robust\nPreprocessing.fit_transform!\nPreprocessing.inv_transform!","category":"page"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.Standard","page":"Preprocessing","title":"HorseML.Preprocessing.Standard","text":"Standard()\n\nStandard Scaler. This scaler scale data as: ildeoldsymbolx = fracx_i-musigma\n\nExample\n\njulia> x = [\n    16.862463771320925 68.10823385851712\n    15.382965696961577 65.4313485700859\n    8.916228406218375 53.92034559524475\n    10.560285659132695 59.17305391117168\n    12.142253214135884 62.28708207525656\n    5.362107221163482 43.604947901567414\n    13.893239446341777 62.44348617377496\n    11.871357065173395 60.28433066289655\n    29.83792267802442 69.22281924803998\n    21.327107214235483 70.15810991597944\n    23.852372696012498 69.81780163668844\n    26.269031430914108 67.61037566099782\n    22.78907104644012 67.78105545358633\n    26.73342178134947 68.59263965946904\n    9.107259141706415 56.565383817343495\n    29.38551885863976 68.1005579469209\n    7.935966787763017 53.76264777936664\n    29.01677894379809 68.69484161138638\n    6.839609488194577 49.69794758177567\n    13.95215840314148 62.058116579899085]; #These data are also used to explanations of other functions.\n\njulia> t = [169.80980778351542, 167.9081124078835, 152.30845618985222, 160.3110300206261, 161.96826472170756, 136.02842285615077, 163.98131131382686, 160.117817321485, 172.22758529098235, 172.21342437006865, 171.8939175591617, 169.83018083884602, 171.3878062674257, 170.52487535026015, 156.40282783981309, 170.6488327896672, 151.69267899906185, 172.32478221316322, 145.14365314788827, 163.79383292080666];\n\njulia> scaler = Standard()\nStandard(Float64[])\n\njulia> fit!(scaler, x)\n2×2 Matrix{Float64}:\n 19.0591   64.467\n  6.95818   6.68467\n\njulia> transform!(scaler, x)\n20×2 Matrix{Float64}:\n  0.0310374   0.44579\n  0.0104337   0.218714\n  1.01139     0.710027\n  1.37285     0.954091\n -0.895893   -0.270589\n  0.983361    0.47397\n  1.39855     0.645624\n -1.31861    -0.901482\n  0.0147702   0.241708\n  0.29675     0.483076\n -0.338824   -0.104812\n  0.432442    0.387922\n  0.860418    0.567095\n -0.495306    0.140871\n  0.963084    0.552767\n -1.38901    -1.05926\n  0.469323    0.719196\n  0.0669475   0.512023\n -1.47583    -1.44017\n -1.99789    -3.27656\n\n\n\n\n\n","category":"type"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.MinMax","page":"Preprocessing","title":"HorseML.Preprocessing.MinMax","text":"MinMax()\n\nMinMax Scaler. This scaler scale data as: tildeboldsymbolx = fracboldsymbolx-min(boldsymbolx)max(boldsymbolx)-min(boldsymbolx)\n\nExample\n\njulia> scaler = MinMax()\nMinMax(Float64[])\n\njulia> fit!(scaler, x)\n2×2 Matrix{Float64}:\n  1.39855   0.954091\n -1.99789  -3.27656\n\njulia> transform!(scaler, x)\n20×2 Matrix{Float64}:\n 0.597368  0.879853\n 0.591301  0.826179\n 0.88601   0.942311\n 0.992432  1.0\n 0.324455  0.710522\n 0.877757  0.886514\n 1.0       0.927088\n 0.199996  0.561398\n 0.592578  0.831614\n 0.675601  0.888666\n 0.488471  0.749707\n 0.715552  0.866175\n 0.841559  0.908526\n 0.442398  0.807779\n 0.871787  0.905139\n 0.179268  0.524104\n 0.72641   0.944478\n 0.607941  0.895508\n 0.153707  0.43407\n 0.0       0.0\n\n\n\n\n\n","category":"type"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.Robust","page":"Preprocessing","title":"HorseML.Preprocessing.Robust","text":"Robust()\n\nRobust Scaler. This scaler scale data as: tildeboldsymbolx = fracboldsymbolx-Q2Q3 - Q1\n\nExample\n\njulia> scaler = Robust()\nRobust(Float64[])\n\njulia> fit!(scaler, x)\n3×2 Matrix{Float64}:\n 0.412913  0.739911\n 0.602654  0.873014\n 0.849116  0.905986\n\njulia> transform!(scaler, x)\n20×2 Matrix{Float64}:\n -0.0121192   0.041181\n -0.0260262  -0.28201\n  0.649595    0.417263\n  0.89357     0.764633\n -0.637774   -0.978423\n  0.630675    0.0812893\n  0.910919    0.3256\n -0.923097   -1.87636\n -0.0230992  -0.249283\n  0.16723     0.0942497\n -0.261766   -0.742477\n  0.258819   -0.041181\n  0.547691    0.213832\n -0.367388   -0.392802\n  0.616988    0.193438\n -0.970618   -2.10092\n  0.283712    0.430314\n  0.0121192   0.135449\n -1.02922    -2.64305\n -1.38159    -5.25675\n\n\n\n\n\n","category":"type"},{"location":"Manual/Preprocessing/#HorseML.Preprocessing.fit_transform!","page":"Preprocessing","title":"HorseML.Preprocessing.fit_transform!","text":"fit_transform!(scaler, x; dims=1)\n\nfit scaler with x, and transform x.\n\n\n\n\n\n","category":"function"},{"location":"#LearningHorse.jl","page":"Home","title":"LearningHorse.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LearningHorse provides an easy-to-use machine learning library","category":"page"},{"location":"#Resources","page":"Home","title":"Resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation\nSource code","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LearningHorse can be installed using the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run.","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add LearningHorse","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"#Manual-indices","page":"Home","title":"Manual indices","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"Manual/Regression.md\",\n    \"Manual/Classification.md\",\n    \"Manual/Preprocessing.md\",\n    \"Manual/LossFunction.md\",\n    \"Manual/NeuralNetwork.md\"\n]\nDepth = 1","category":"page"},{"location":"#Tutorial-indices","page":"Home","title":"Tutorial indices","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"Tutorial/Welcome.md\",\n    \"Tutorial/Getting_Started.md\",\n    \"Tutorial/Preprocessing.md\",\n    \"Tutorial/Classifiers.md\",\n    \"Tutorial/Tree.md\",\n    \"Tutorial/NeuralNetwork.md\"\n]\nDepth = 1","category":"page"}]
}
